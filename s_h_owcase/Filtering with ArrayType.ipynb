{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Abstract\n",
    "\n",
    "The purpose of this notebook is to explore on arrayType filtering to replace wildcard filtering. We desire this because this might enhance the performance of our algorithms against a large set of cutomer table and offer table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Random Table Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined trait Dummy_Data_Generator\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scala.util.Random\n",
    "import math.{ round, min, max }\n",
    "\n",
    "trait Dummy_Data_Generator {\n",
    "    \n",
    "    /**\n",
    "     * Simulate a double of range 0 (inclusive) to `value` (exclusive).\n",
    "     */\n",
    "    def random_double(\n",
    "        rand: Random = new Random, \n",
    "        value: Double = 1): Double = {\n",
    "        rand.nextDouble * value\n",
    "    }\n",
    "    \n",
    "    /**\n",
    "     * Randomly select some items (size less than or equal to `max_item` but greater than or\n",
    "     * equal to `min_item`) from the given array and output a string with items seperated\n",
    "     * by `sep`.\n",
    "     */\n",
    "    def random_array_to_string[T](\n",
    "        rand: Random = new Random, \n",
    "        array: Array[T],\n",
    "        min_item: Int = 1,\n",
    "        max_item: Int = 3,\n",
    "        sep: String = \",\",\n",
    "        duplicate: Boolean = false,\n",
    "        prob_array: Array[Double] = Array()): String = {\n",
    "        \n",
    "      if(array.isEmpty)\n",
    "        throw new Exception(\"Invalid configuration: simulate from empty array\")\n",
    "      else if (prob_array.length != array.length && !prob_array.isEmpty)\n",
    "        throw new Exception(\"Invalid configuration: different length of prob_array and array.\")\n",
    "      else {     \n",
    "        val len: Int = array.length\n",
    "        \n",
    "        // Declare output variable.\n",
    "        var output = \"\" + sep  \n",
    "          \n",
    "        // If prob_array is not provided, then assume equal probability for each item.\n",
    "        lazy val prob_each: Double = 1.0 / len \n",
    "        val probArray: Array[Double] = \n",
    "          if (prob_array.isEmpty) array.map(x => prob_each) else prob_array\n",
    "        \n",
    "        // Number of items that will be in the list\n",
    "        var num = max(min(rand.nextInt(len), max_item), min_item)\n",
    "          \n",
    "        // Tail recursive method of simulating from `array`.\n",
    "        def gen[T](array: Array[T], probArray: Array[Double], sim: Double): T = {\n",
    "          if (array.length == 1 || sim <= probArray.head)\n",
    "            array.head\n",
    "          else\n",
    "            gen(array.tail, probArray.tail, sim - probArray.head)        \n",
    "        }\n",
    "        \n",
    "        // Simulate a random double from 0 (inclusive) to `probArray.sum` (exclusive).\n",
    "        var sim = 0.5\n",
    "          \n",
    "        // Generate the output list  \n",
    "        while (num > 0) {\n",
    "            sim = random_double(rand, probArray.sum)\n",
    "            output = output + \n",
    "                gen(array.tail, probArray.tail, sim - probArray.head).toString + sep\n",
    "            num = num - 1\n",
    "        }\n",
    "        \n",
    "        // Remove the first and last element of the output, which are `sep`'s.\n",
    "        output.drop(1).dropRight(1)\n",
    "      }\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Dummy_Table_Generator\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.{ SparkConf, SparkContext }\n",
    "import org.apache.spark.sql.{ SparkSession, SQLContext, Row }\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import scala.util.Random \n",
    "\n",
    "class Dummy_Table_Generator(spark: SparkSession, rand: Random) extends Dummy_Data_Generator {\n",
    "    \n",
    "  // Generate sparkContext from sparkSession.\n",
    "  val sc = spark.sparkContext\n",
    "    \n",
    "  /**\n",
    "   * Simulate a random table with `num_row` rows according to `col_schema` and `col_map`.\n",
    "   */\n",
    "  def random_table(num_row: Int, col_schema: Array[StructField], col_map: Int => Row) = {\n",
    "    import spark.implicits._\n",
    "      \n",
    "    val rdd = sc.makeRDD(Range(1, num_row + 1).map(col_map))\n",
    "    spark.createDataFrame(rdd, StructType(col_schema))\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@16ccc45b\n",
       "rand = scala.util.Random@5eada09e\n",
       "table_gen = Dummy_Table_Generator@7b2694f\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dummy_Table_Generator@7b2694f"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Set up a SparkSession object.\n",
    "val spark = SparkSession.builder\n",
    "  .master(\"local[*]\")\n",
    "  .appName(\"Dummy Table Generator Example 1\")\n",
    "  .config(\"spark.some.config.option\", \"some-value\")\n",
    "  .getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "// Set seed for random number generator.\n",
    "val rand = new Random(588)\n",
    "\n",
    "// Table generator\n",
    "val table_gen = new Dummy_Table_Generator(spark, rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Customer Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|Cust_ID|Cust_Desc  |\n",
      "+-------+-----------+\n",
      "|100001 |L2,L20,L27 |\n",
      "|100002 |L9,L30,L17 |\n",
      "|100003 |L27,L3,L4  |\n",
      "|100004 |L21,L26,L6 |\n",
      "|100005 |L21,L17,L13|\n",
      "|100006 |L8,L15,L3  |\n",
      "|100007 |L16,L10,L5 |\n",
      "|100008 |L3,L3,L9   |\n",
      "|100009 |L28,L30,L22|\n",
      "|100010 |L17,L19,L29|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "descList = Array(L1, L2, L3, L4, L5, L6, L7, L8, L9, L10, L11, L12, L13, L14, L15, L16, L17, L18, L19, L20, L21, L22, L23, L24, L25, L26, L27, L28, L29, L30)\n",
       "custColSchema = Array(StructField(Cust_ID,IntegerType,true), StructField(Cust_Desc,StringType,true))\n",
       "custColMap = > org.apache.spark.sql.Row = <function1>\n",
       "customer = [Cust_ID: int, Cust_Desc: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Cust_ID: int, Cust_Desc: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// List of customer descriptions\n",
    "val descList = Array(\"L1\", \"L2\", \"L3\", \"L4\", \"L5\", \"L6\", \"L7\", \"L8\", \"L9\", \"L10\",\n",
    "                     \"L11\", \"L12\", \"L13\", \"L14\", \"L15\", \"L16\", \"L17\", \"L18\", \"L19\", \"L20\",\n",
    "                     \"L21\", \"L22\", \"L23\", \"L24\", \"L25\", \"L26\", \"L27\", \"L28\", \"L29\", \"L30\")\n",
    "\n",
    "// Column schema for the customer table.\n",
    "val custColSchema = Array(\n",
    "    StructField(\"Cust_ID\", IntegerType, true),\n",
    "    StructField(\"Cust_Desc\", StringType, true))\n",
    "\n",
    "// Column mapping for the customer table.\n",
    "val custColMap = (x: Int) => Row(\n",
    "    100000 + x,\n",
    "    table_gen.random_array_to_string(rand, descList, 0, 3))\n",
    "\n",
    "// The customer table.\n",
    "val customer = table_gen.random_table(10, custColSchema, custColMap)\n",
    "\n",
    "customer.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Offer Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|Offer_ID|Cust_Desc_from_Offer|\n",
      "+--------+--------------------+\n",
      "|1001    |L30,L25             |\n",
      "|1002    |L4,L16              |\n",
      "|1003    |L16,L26             |\n",
      "|1004    |L15                 |\n",
      "|1005    |L29,L29             |\n",
      "|1006    |L9,L2               |\n",
      "|1007    |L28,L4              |\n",
      "|1008    |L8,L8               |\n",
      "|1009    |L27,L2              |\n",
      "|1010    |L21                 |\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "offerColSchema = Array(StructField(Offer_ID,IntegerType,true), StructField(Cust_Desc_from_Offer,StringType,true))\n",
       "offerColMap = > org.apache.spark.sql.Row = <function1>\n",
       "offer = [Offer_ID: int, Cust_Desc_from_Offer: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Offer_ID: int, Cust_Desc_from_Offer: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Offer schema for the offer table.\n",
    "val offerColSchema = Array(\n",
    "    StructField(\"Offer_ID\", IntegerType, true),\n",
    "    StructField(\"Cust_Desc_from_Offer\", StringType, true))\n",
    "\n",
    "// Offer mapping for the offer table.\n",
    "val offerColMap = (x: Int) => Row(\n",
    "    1000 + x,\n",
    "    table_gen.random_array_to_string(rand, descList, 1, 2))\n",
    "\n",
    "// The offer table.\n",
    "val offer = table_gen.random_table(10, offerColSchema, offerColMap)\n",
    "\n",
    "offer.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. ArrayType Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------------+\n",
      "|Cust_ID|Cust_Desc  |Cust_Desc_Array|\n",
      "+-------+-----------+---------------+\n",
      "|100001 |L2,L20,L27 |[L2, L20, L27] |\n",
      "|100002 |L9,L30,L17 |[L9, L30, L17] |\n",
      "|100003 |L27,L3,L4  |[L27, L3, L4]  |\n",
      "|100004 |L21,L26,L6 |[L21, L26, L6] |\n",
      "|100005 |L21,L17,L13|[L21, L17, L13]|\n",
      "|100006 |L8,L15,L3  |[L8, L15, L3]  |\n",
      "|100007 |L16,L10,L5 |[L16, L10, L5] |\n",
      "|100008 |L3,L3,L9   |[L3, L3, L9]   |\n",
      "|100009 |L28,L30,L22|[L28, L30, L22]|\n",
      "|100010 |L17,L19,L29|[L17, L19, L29]|\n",
      "+-------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Demonstration of arrayType in Spark Dataframe\n",
    "customer.withColumn(\"Cust_Desc_Array\", split(col(\"Cust_Desc\"), \"\\\\,\")).show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "custDescIntersect = UserDefinedFunction(<function2>,BooleanType,Some(List(ArrayType(StringType,true), ArrayType(StringType,true))))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UserDefinedFunction(<function2>,BooleanType,Some(List(ArrayType(StringType,true), ArrayType(StringType,true))))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.mutable.WrappedArray\n",
    "\n",
    "// User defined function to check intersection.\n",
    "val custDescIntersect = udf {\n",
    "    (dfArray1: WrappedArray[String], dfArray2: WrappedArray[String]) => \n",
    "        ((dfArray1.toList.intersect(dfArray2.toList)).size > 0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------+--------------------+---------+\n",
      "|Cust_ID|Cust_Desc |Offer_ID|Cust_Desc_from_Offer|Intersect|\n",
      "+-------+----------+--------+--------------------+---------+\n",
      "|100001 |L2,L20,L27|1001    |L30,L25             |false    |\n",
      "|100001 |L2,L20,L27|1002    |L4,L16              |false    |\n",
      "|100002 |L9,L30,L17|1001    |L30,L25             |true     |\n",
      "|100002 |L9,L30,L17|1002    |L4,L16              |false    |\n",
      "|100001 |L2,L20,L27|1003    |L16,L26             |false    |\n",
      "|100001 |L2,L20,L27|1004    |L15                 |false    |\n",
      "|100001 |L2,L20,L27|1005    |L29,L29             |false    |\n",
      "|100002 |L9,L30,L17|1003    |L16,L26             |false    |\n",
      "|100002 |L9,L30,L17|1004    |L15                 |false    |\n",
      "|100002 |L9,L30,L17|1005    |L29,L29             |false    |\n",
      "|100001 |L2,L20,L27|1006    |L9,L2               |true     |\n",
      "|100001 |L2,L20,L27|1007    |L28,L4              |false    |\n",
      "|100002 |L9,L30,L17|1006    |L9,L2               |true     |\n",
      "|100002 |L9,L30,L17|1007    |L28,L4              |false    |\n",
      "|100001 |L2,L20,L27|1008    |L8,L8               |false    |\n",
      "|100001 |L2,L20,L27|1009    |L27,L2              |true     |\n",
      "|100001 |L2,L20,L27|1010    |L21                 |false    |\n",
      "|100002 |L9,L30,L17|1008    |L8,L8               |false    |\n",
      "|100002 |L9,L30,L17|1009    |L27,L2              |false    |\n",
      "|100002 |L9,L30,L17|1010    |L21                 |false    |\n",
      "+-------+----------+--------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tgt = [Cust_ID: int, Cust_Desc: string ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Cust_ID: int, Cust_Desc: string ... 5 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Find intersection.\n",
    "val tgt = \n",
    "  customer.withColumn(\"Cust_Desc_Array\", split(col(\"Cust_Desc\"), \"\\\\,\"))\n",
    "    .crossJoin(offer.withColumn(\"Cust_Desc_Offer_Array\",\n",
    "                                split(col(\"Cust_Desc_from_Offer\"), \"\\\\,\")))\n",
    "    .withColumn(\"Intersect\", custDescIntersect($\"Cust_Desc_Array\", $\"Cust_Desc_Offer_Array\"))\n",
    "\n",
    "tgt.selectExpr(\n",
    "    \"Cust_ID\",\n",
    "    \"Cust_Desc\",\n",
    "    \"Offer_ID\",\n",
    "    \"Cust_Desc_from_Offer\",\n",
    "    \"Intersect\").show(20, false)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
