{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford Machine Learning Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tom Mitchell: \"A computer program is said to learn from experience E with respect to some class of task T and performance measure P, if its performance in task T, measured by P, improves with experience E.\"\n",
    "\n",
    "Taking the classification of spam emails as an example,\n",
    "\n",
    "    T: classify/identify spam emails;\n",
    "    E: watch/observe how a human classify spam emails;\n",
    "    P: proportion of spam emails that are identified by our algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning usually consists of problems which we assume the form of the answer. For instance, in the classification of spam email example, we want an algorithm that can output 'spam' or 1 if the email is indeed a spam email, and 0 otherwise. This is a classification problem (problem with discrete output). Another form of supervised learning is regression (problem with continous output), such as predicting the weight of students given the height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "In supervised learning, out data set given is usually labelled. For instance, back to our spam email example, the training data usually will infer which emails are spam and which emails are not. In unsupervised learning however, the data are usually more raw (and unlabelled) and we often let algorithm itself to identify the structure of the data. Of cause, we usually won't be knowing the form of result we will achieve beforehand. \n",
    "\n",
    "For instance, we can seperate multiple people's voices from audios with unsupervised learning, or group customers using clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "$\\left(x^{(i)}, y^{(i)}\\right)$: A tuple describing the $i$-th observation of the training data.\n",
    "\n",
    "$m$: Number of observations in the training data.\n",
    "\n",
    "$\\theta$ : Model parameter.\n",
    "\n",
    "$h_{\\theta}(x)$: The hypothesis function (the predicted value given input `x`).\n",
    "\n",
    "$J(\\boldsymbol{\\theta})$: The cost function: e.g. (halfed) mean squared error $\\frac{1}{2m} \\sum_{i = 1}^m \\left( h_{\\theta} (x^{(i)}) - y^{(i)} \\right)^2$.\n",
    "\n",
    "> In computer science language (and mathematics), `:=` represent assign to. For instance, `a := 2` means we assign the value 2 to `a`. Note that this is not the same as =, which represent true assertion: `a = a + 1` does not make sense unless `a` is 0, but we can use `a := a + 1` to indicate we increase the value of `a` by 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Decent Algorithm\n",
    "\n",
    "In the simple linear regression case, we repeat the following for $j = 0, 1$ until convergence:\n",
    "\n",
    "$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1)$,\n",
    "\n",
    "where $\\alpha$ is the learning rate, as it controls how big is each step of decent.\n",
    "\n",
    "Note that, for correct implementation, we need to update both parameters simultanously:\n",
    "\n",
    "> `temp0` := $\\theta_0 - \\alpha \\frac{\\partial}{\\partial \\theta_0} J(\\theta_0, \\theta_1)$<br>\n",
    "`temp1` := $\\theta_1 - \\alpha \\frac{\\partial}{\\partial \\theta_0} J(\\theta_0, \\theta_1)$<br>\n",
    "$\\theta_0$ := `temp0`<br>\n",
    "$\\theta_1$ := `temp1`.\n",
    "\n",
    "Note that if we swap the second and the third line (i.e. non-simultanous update, as we update them one at a time), it might still get us the local optimium, but we can no longer call the algorithm gradient decent. Hence, when people mention gradient decent algorithm, they generally refer to the implementation with simultanous update.\n",
    "\n",
    "This example is actually called 'Batch' gradient decent, since in every update, we are using all of our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on Notation\n",
    "\n",
    "$x_j^{(i)}$ represent the $i$-th observation value of the $j$-th covariate.\n",
    "\n",
    "For convenience of notation, $x_0^{(i)} = 1$ for all $i$.\n",
    "\n",
    "Parameters: $\\boldsymbol{\\theta} = (\\theta_0, \\theta_1, \\ldots, \\theta_k)^T \\in \\mathbb{R}^{k+1}$\n",
    "\n",
    "Hypothesis: $h_{\\theta} (\\boldsymbol{x}) = \\boldsymbol{\\theta}^T \\boldsymbol{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Decent Algorithm for Multivariate Linear Regression\n",
    "\n",
    "For $j$ = 0, 1, 2, ..., k, upate simultanously: \n",
    "\n",
    "$$\\theta_j := \\theta_j + \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\boldsymbol{\\theta}) = \\theta_j + \\alpha \\sum_{i=1}^m \\left(h_{\\theta} \\left(x^{(i)}\\right) - y^{(i)}\\right) x_j^{(i)}$$.\n",
    "\n",
    "Note that $m$ is incorporated into the learning rate $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "*Idea: make sure features are on a similar scaling.*\n",
    "\n",
    "If features are on different scaling, then it is possible for some features to reach local optinium quickly, while others taking a long time to reach convergence.\n",
    "\n",
    "Consequently, we desire $-1 \\leq x_j \\leq 1$, for all $j$'s, and recall that $x_0 = 1$.\n",
    "\n",
    "To achieve this, we can set\n",
    "\n",
    "$$x_i := \\frac{x_i}{s_i},$$\n",
    "\n",
    "where $s_i$ is the range of $x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean normalisation\n",
    "\n",
    "*Idea: to make the features to have approximately zero mean.*\n",
    "\n",
    "$$x_i := \\frac{x_i - \\mu_i}{s_i},$$\n",
    "\n",
    "where $\\mu_i$ is the mean of $x_i$ and $s_i$ is the range of $x_i$ or the standard deviation of $x_i$.\n",
    "\n",
    "If we were to use the range of $x_i$, then this modification can scale the range of $x_i$ to [-0,5, 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate\n",
    "\n",
    "Choosing the learning rate is important, if the learning rate is too large, then the gradient decent algorithm may fail to converge, and if the learning rate is too small, the algorithm may take too long to converge.\n",
    "\n",
    "Mathematician has shown that for sufficently small $\\alpha$, the cost function $J(\\boldsymbol{\\theta})$ will always be decreasing on every iteration.\n",
    "\n",
    "Pratically, we would plot the cost function $J(\\boldsymbol{\\theta})$ against number of iterations. And we would try a range of learning rate $\\alpha$, for instance\n",
    "\n",
    "$$\\ldots, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, \\ldots$$\n",
    "\n",
    "We can use **automatic convergence test** to signify convergence, e.g. if the change in cost function is less than $10^{-3}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Equation and Comparision\n",
    "\n",
    "Using vector calculus, we can show that the solution to the least square problem (our task of finding $\\boldsymbol{\\theta}$ as a minimiser to our quadratic cost function $J(\\boldsymbol{\\theta})$ given the design matrix $X = (\\boldsymbol{1}, \\boldsymbol{x}_1, \\ldots, \\boldsymbol{x}_k)^T$ and response $\\boldsymbol{y}$) can be represented as\n",
    "$$\\boldsymbol{\\theta} = \\left(X^T X\\right)^{-1} X^T \\boldsymbol{y}.$$\n",
    "This is called the **normal equation**.\n",
    "\n",
    "This gives us the exact local optimium value for $\\boldsymbol{\\theta}$ without iterations, plotting cost functions and setting learning rates. Therefore, this is much more efficient when the size of parameter $k$ is small. However, when the size of parameter $k$ is very large (e.g. $10^6$), then computing matrix inverse becomes time consuming and gradient decent algorithm becomes more time-efficent.\n",
    "\n",
    "As a rule of thumb, we would start considering swtiching from normal equation to gradient decent algorithm when $k$ exceeds $10^4$.\n",
    "\n",
    "*Octave code:*\n",
    "> `pinv(X' * X) * X' * y`: note that `pinv` is pesudo inverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Octave Basic Operations\n",
    "\n",
    "Note that Octave is an open source version of `MATLAB`, hence the commands are fairly similar.\n",
    "\n",
    "> `~=`: not equal. (note that some languages such as `R` uses `!=` as not equal.)<br>\n",
    "`&&`: logic operatoin for `AND`.<br>\n",
    "`||`: logic operation for `OR`.<br>\n",
    "`PS1('>> ')`: changes the shown prompt to `>>` in the command window.<br>\n",
    "`;`: suppress output.<br>\n",
    "`.*`: does element-wise operation. `*` does matrix operation. (repleace * by the operation you want to perform.)\n",
    "\n",
    "Woking with variables and matrices:\n",
    "> `ones(m, n)`: generates m by n matrix of ones.<br>\n",
    "`zeros(m, n)`: generate m by n matrix of zeros.<br>\n",
    "`eye(n)`: generate n by n identity matrix.<br>\n",
    "`rand(m, n)`: generate m by n matrix of random uniform.<br>\n",
    "`randn(m, n)`: generate m by n matrix of random gaussian (standard normal).<br>\n",
    "`size(x)`: checks the dimension of the vairbale `x`.<br>\n",
    "`x(a : b)`: displays the a-th element to b-th element of vector `x`.<br>\n",
    "`A(i, j)`: displace the element in the i-th row and j-th column of matrix `A`.<br>\n",
    "`A(:)`: put all elements in `A` in one column vector.<br>\n",
    "`[val ind] = max(x)`: returns the value and the index for the maximum of `x`.<br>\n",
    "`max(A, [], 1)`: computes the maximum of `A` by row. `max(A, [], 2)` computes the max of `A` by column.<br>\n",
    "`flipud(A)`: flip up down the matrix `A`. We can use `A .* eye(length(A))` and `A .* flipud(eye(length(A)))` to find the diagonal sum.\n",
    "\n",
    "\n",
    "Checking variables in the current workplace:\n",
    "> `who`: displays what variables are in the current workplace.<br>\n",
    "`whos`: displays what variables are in the current workplace with details.<br>\n",
    "`clear x`: removes variable `x` from the current workplace.<br>\n",
    "`clear`: removes all variables from the current workplace.\n",
    "\n",
    "To change working directory:\n",
    "> `pwd`: shows the current directory/path of the octave program.<br>\n",
    "`cd '...'`: changes the directory to `...`.\n",
    "\n",
    "To create/save fils:\n",
    "> `save name.mat x`: save `x` as file `name.mat` in the working directory.<br>\n",
    "> `save name.txt x -ascii`: save `x` as text (ASCII) file.\n",
    "\n",
    "To load data:\n",
    "> `load('filename.dat')`: loads data with file name `filename.dat`. We can also do `load filename.dat`.\n",
    "\n",
    "To plot data:\n",
    "> `plot(x, y)`: plot `y` against `x`.<br>\n",
    "`hold on`: allows octave to plot another function above the current plot.<br>\n",
    "`xlabel()` and `ylabel()` allow us to label the horizontal and vertical axes.<br> \n",
    "`legend()`: allows us to add legend to the graph.<br>\n",
    "`title()`: adds title to the plot.<br>\n",
    "`print -dpng 'name.png'`: saves the plot as png file `name.png`. Use `help plot` to check how to save the plot in other formats.<br>\n",
    "`subplot(1,2,1)`: divides the plot into 1 by 2 grid and access the first element.<br>\n",
    "`axis([a b c d])`: changes the margin of `x` to [a, b] and `y` to [c, d].\n",
    "\n",
    "Functions: \n",
    "> `addpath(...)`: adds the path `...` to octave's search path.<br>\n",
    "`function [a, b] = f(x)\n",
    "    a = 1; b = 1;`: octave allows you to return two values of output.<br>\n",
    "    \n",
    "Cost function example:\n",
    "> `function j = J(X, y, theta)\n",
    "    m = size(X,1);\n",
    "    pred = X * theta;\n",
    "    sqerror = (pred - y) .^ 2;\n",
    "    j = 1 / (2 * m) * sum(sqerror);`\n",
    "\n",
    "For assignment purposes:\n",
    "> `setenv(\"GNUTERM\",\"qt\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Extension) Vector/Matrix Calculus Representation\n",
    "\n",
    "If you are familiar with vector and matrix calculus, then we can represent the cost function as \n",
    "\n",
    "$$J(\\boldsymbol{\\theta}) = \\frac{1}{2 m} \\left(X \\boldsymbol{\\theta} - \\boldsymbol{y}\\right)^T \\left(X \\boldsymbol{\\theta} - \\boldsymbol{y}\\right),$$\n",
    "\n",
    "and the update process of gradient decent algorithm is\n",
    "\n",
    "$$\\boldsymbol{\\theta} := \\boldsymbol{\\theta} - \\frac{\\alpha}{m} X^T\\left(X \\theta - y\\right).$$\n",
    "\n",
    "Vectorisation is more desired comparing to loops, since it is generally more efficent and faster, espicially when the size of $X$ is very large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "In classification problems, using linear regression is not desired. For instance, in the case of binary response (i.e. the response is either 0 or 1), linear regression can output values that are much greater than 1 or a very negative number. To restrict the output, we use logistic regression.\n",
    "\n",
    "To ensure $0 \\leq h_{\\theta} (x) \\leq 1$, we use the Sigmoid function (or sometimes called logistic function, or the cumulative distribution function of logistic distribution in statistics):\n",
    "$$g(z) = \\frac{1}{1 + e^{-z}},$$\n",
    "so that we can write\n",
    "$$h_{\\theta} (x) = g \\left( \\theta^T x \\right) = \\frac{1}{1 + e^{-\\theta^T x}}.$$\n",
    "The interpretation of this hypothesis is \n",
    "$$h_{\\theta} (x) = \\mathbb{P} \\left( y = 1 \\mid x ; \\theta \\right) = \\text{probability of y = 1, given x, parameterised by }\\theta.$$\n",
    "\n",
    "### Proof (Extension):\n",
    "Note that the proof requires certain degree of statistical knowledge.\n",
    "\n",
    "In logistic regression, we assume there exists a latent (unobservable) variable $y^*$ such that the value of $y$ depends on the value of $y^*$:\n",
    "> If $y^*$ > 0, then $y$ = 1. Otherwise $y$ = 0.\n",
    "\n",
    "Then we can model $y^*$ using \n",
    "> $y^* = \\theta^T x + \\epsilon$,<br>\n",
    "where the error term $\\epsilon \\mid x \\sim \\mathcal{L}ogis \\left(0, \\frac{\\pi^2}{3} \\right)$ follows logistic distribution with CDF $\\mathbb{P}(\\epsilon \\leq z \\mid x) = \\frac{1}{1 + e^{-z}}$.\n",
    "\n",
    "Therefore, we have\n",
    "> $\\mathbb{P}(y = 1 \\mid x; \\theta) = \\mathbb{P}(y^* > 0 \\mid x; \\theta) = \\mathbb{P}(\\epsilon \\geq - \\theta^T x \\mid x; \\theta) = \\mathbb{P}(\\epsilon \\leq \\theta^T x \\mid x; \\theta) = \\frac{1}{1 + e^{-\\theta^T x}} = g(\\theta^T x) = h_{\\theta} (x).$\n",
    "\n",
    "Note that the third equality uses the symmetry property of logistic distribution. Additionally, since $\\theta$ can be a random variable (e.g. in Bayesian inference), so by conditioning on $\\theta$ we can use the CDF of logistic distribution in the second equality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Boundry\n",
    "\n",
    "In order to achieve binary output, we set\n",
    "> $y = 1$ if $h_{\\theta} (x) \\geq 0.5$<br>\n",
    "$y = 0$ if $h_{\\theta} (x) < 0.5$.\n",
    "\n",
    "Since we know $g(z) \\geq 0.5$ if $z \\geq 0$, we have\n",
    "> $y = 1$ if $\\theta^T x \\geq 0$<br>\n",
    "$y = 0$ if $\\theta^T x < 0$.\n",
    "\n",
    "Hence we can draw decision boundry (the line where $\\theta^T x = 0$) to do the classification. By increasing the complexity of $\\theta$, such as using polynomial regression, we can draw a complex decision boundry to help us classify the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function for Logistic Regression\n",
    "\n",
    "Unfortunately, quadratic loss function will become non-convex in the case of logistic regression, which hinders the ability for gradient decent algorithms to find the global optinium. Hence, we have to use an alternative cost function:\n",
    "> $J(\\theta) = \\frac{1}{m} \\text{Cost} \\left( h_{\\theta} \\left(x^{(i)}\\right), y^{(i)}\\right),$ where<br>\n",
    "$\\text{Cost} \\left( h_{\\theta} \\left(x\\right), y\\right) = - \\log \\left( h_{\\theta} (x) \\right)$ if $y = 1$<br>\n",
    "$\\text{Cost} \\left( h_{\\theta} \\left(x\\right), y\\right) = - \\log \\left( 1 - h_{\\theta} (x) \\right)$ if $y = 0$\n",
    "\n",
    "Note that if $y = 1$, $\\text{Cost} \\left( h_{\\theta} \\left(x\\right), y\\right)$ goes to infinity when $h_{\\theta} \\left(x\\right)$ goes to 0, and equals to 0 when $h_{\\theta} \\left(x\\right)$ is 1, which is fairly ideal for the cost function that we are looking for.\n",
    "\n",
    "### (Extension) Derivation of the cost function\n",
    "\n",
    "This requires some understanding about the maximum likelihood optimisation in statistics. Maximum likelihood is a standard procedure to find the most optimium parameters for a given model in the field of statistics. It aims to maximise the likelihood function, which describes how likely it is to observe the data given the parameters.\n",
    "\n",
    "Note that, in the following we will use standard statistical notation, so that\n",
    "> we use $x_i$ to represent $x^{(i)}$, and $y_i$ to represent $y^{(i)}$, since it is easier for me to write those in latex.\n",
    "\n",
    "The likelihood function in logistic regression is \n",
    "$$\\mathcal{L}(\\boldsymbol{\\theta};\\boldsymbol{x}) = \\prod_{i; y_i = 1} \\mathbb{P}(y_i = 1 \\mid x_i, \\theta) \\prod_{i; y_i = 0} \\mathbb{P}(y_i = 0 \\mid x_i, \\theta) = \\prod_{i; y_i = 1} h_{\\theta} (x_i) \\prod_{i; y_i = 0} \\left(1 - h_{\\theta} (x_i) \\right).$$\n",
    "\n",
    "Since maximising the likelihood function is the same as maximising the log-likelihood function, we shall derive the log-likelihood function to be \n",
    "$$ \\ell(\\boldsymbol{\\theta}; \\boldsymbol{x}) = \\log \\mathcal{L}(\\boldsymbol{\\theta}; \\boldsymbol{x}) = \\sum_{i; y_i = 1} \\log (h_{\\theta}(x_i)) + \\sum_{i; y_i = 0}\\log ( 1 - h_{\\theta}(x_i)).$$\n",
    "\n",
    "Now, maximising the log-likelihood function is the same as minimising the negative of the log-likelihood, and scaling the function by $\\frac{1}{m}$ will not affect out final answer. Hence, we have our new cost function:\n",
    "$$J(\\theta) = -\\frac{1}{m}\\left( \\sum_{i; y_i = 1} \\log (h_{\\theta}(x_i)) + \\sum_{i; y_i = 0}\\log ( 1 - h_{\\theta}(x_i)) \\right).$$\n",
    "\n",
    "### Simplifying the cost function\n",
    "\n",
    "With some simple mathematical manipulation, we can rewrite the cost function as \n",
    "$$\\text{Cost} \\left( h_{\\theta} \\left(x\\right), y\\right) = - y \\log \\left( h_{\\theta} (x) \\right) - (1 - y) \\log \\left( 1 - h_{\\theta} (x) \\right),$$\n",
    "so that we have\n",
    "$$J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^m \\left(y^{(i)}  \\log \\left( h_{\\theta} \\left(x^{(i)}\\right) \\right) + \\left(1 - y^{(i)}\\right) \\log \\left( 1 - h_{\\theta} \\left(x^{(i)}\\right) \\right)\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Decent for Logistic Regression\n",
    "\n",
    "Recall that $h_{\\theta} (x) = \\frac{1}{1 + \\exp \\left(-\\theta^T x\\right)}$, then $\\frac{\\partial}{\\partial \\theta_j} h_{\\theta} (x) = \\frac{x \\exp \\left(-\\theta^T x\\right)}{\\left(1 + \\exp \\left(-\\theta^T x\\right)\\right)^2} = x_j h_{\\theta}(x) (1 - h_{\\theta} (x))$. Hence,\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m -y^{(i)}x_j^{(i)}\\left(1 - h_{\\theta}\\left(x^{(i)}\\right)\\right) + \\left(1 - y^{(i)}\\right)x_j^{(i)}h_{\\theta}\\left( x^{(i)} \\right).$$\n",
    "\n",
    "Simplifying gives\n",
    "$$\\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\left(h_{\\theta} \\left(x^{(i)}\\right) - y^{(i)}\\right)x^{(i)}_j.$$\n",
    "\n",
    "Hence to apply gradient decent, we simultanously update\n",
    "> $\\theta_j := \\theta_j - \\alpha \\sum_{i=1}^m \\left(h_{\\theta} \\left(x^{(i)}\\right) - y^{(i)}\\right)x^{(i)}_j$\n",
    "\n",
    "for $j=0, 1, \\ldots, p$.\n",
    "\n",
    "Note that this algorithm looks almost identical to the one of linear regression. However, we must realise that even though the rule is the same, the definition of hypothesis and the cost function is different.\n",
    "\n",
    "We can still apply feature scaling to gradient decent algorithm for logistic regression to speed up the rate of conversion.\n",
    "\n",
    "### Vectorised Version of the Updating Process\n",
    "\n",
    "Similar to the case in linear regression, we can update $\\boldsymbol{\\theta}$ by\n",
    "$$\\boldsymbol{\\theta} := \\boldsymbol{\\theta} - \\frac{\\alpha}{m} X^T \\left(g\\left(X \\boldsymbol{\\theta} \\right) - \\boldsymbol{y}\\right),$$\n",
    "where $g$ is the Sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Optimisation Method\n",
    "\n",
    "The method is outside the scope of the course, these are more complex optimisation method than gradient decent and often converges much faster.\n",
    "\n",
    "### Octave Code\n",
    "\n",
    "We first write down the expression for the cost function its partial derivatives.\n",
    "> `function [jVal, gradient] = costFunction(theta)\n",
    "      jVal = [...code to compute J(theta)...];\n",
    "      gradient = [...code to compute derivative of J(theta)...];\n",
    "end`\n",
    "\n",
    "Then, we can apply advanced unconstrained optimisation method to find $\\theta$.\n",
    "> `options = optimset('GradObj', 'on', 'MaxIter', 100);\n",
    "initialTheta = zeros(2,1);\n",
    "   [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification\n",
    "\n",
    "Suppose we have $n+1$ classes for the responses, i.e. $y \\in \\{0, 1, \\ldots, n\\}$. Then for $j = 0, 1, \\ldots, n$, we construct new label $z$ where $z_i = 1$ if $y_i = j$, and $z_i = 0$ otherwise (essentially, we are treating all classes other than class $j$ as one class) and fit a logistic regression. This gives us $h_{\\theta}^{(j)} = \\mathbb{P}(y = j \\mid x; \\theta)$ for $j = 0, 1, \\ldots, n$. And our prediction will be the most likely outcome:\n",
    "$$h_{\\theta} (x) = \\max_j h^{(j)}_{\\theta} (x).$$\n",
    "\n",
    "So for a $n+1$ class classification problem, we are fitting $n+1$ logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem of Overfitting\n",
    "\n",
    "Overfitting occurs when the model are more focused on describing the noise, rather than the trend. It usually performs well in predicting the examples in the training set, but fails at predicting new examples from the same data generating process. Since machine learning is more emphasised on the accuracy of prediction (of new and training data), hence overfitting is something that we have to tbe careful of.\n",
    "\n",
    "To define overfitting more rigorously, we need some statistical knowledge.\n",
    "\n",
    "### (Extension) Bias-Variance Trade-off\n",
    "\n",
    "In regression problems, we assume all data are samples from the regression function $r$, and our goal is to find the best estimator $\\hat{r}_n$ to describe the data generating process. Mean squared error (MSE) is defined as\n",
    "$$\\text{MSE} (x) = \\mathbb{E}\\left[((\\hat{r}_n (x) - r(x))^2\\right] = \\left(\\mathbb{E}[\\hat{r}_n (x)] - \\hat{r}_n (x)\\right)^2+ \\mathbb{V}\\text{ar}(\\hat{r}_n (x)) = \\text{Bias}^2 + \\mathbb{V}\\text{ar}(\\hat{r}_n (x)).$$\n",
    "\n",
    "Bias is the difference between the expected value of the estimator and its 'true' value, while variance refers to how variable is the estimator relatively to its mean.\n",
    "\n",
    "Ideally, we want the estimator to have low bias and low variance to have a small MSE. However, at certain point, decreasing bias often lead to an increase in variance and vice versa. Therefore, we have to balance the bias and variance to choose a good model, this is known as the bias-variance trade-off.\n",
    "\n",
    "**Overfitting** refers to the scenario when the estimator has small bias but large variance, while **underfitting** refers to the scenario when the estimator has large bias but small variance. (To get a better picture, you can search up in Google to find overfitted and underfitted examples.)\n",
    "\n",
    "Overfitting often occurs when there are too many features in the model. We can either deal with it by reducing the number of features through model selection algorithm (will be covered later in the course) or through regularisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularisation\n",
    "\n",
    "One way to address overfitting is through regularisation, where we penalise all parameters in order to favor parsimonious (simple and efficient) models. We can do so by changing our cost function to\n",
    "$$J(\\theta) = \\frac{1}{2m} \\sum_{i = 1}^{m}\\left(h_{\\theta} \\left(x^{(i)}\\right) - y^{(i)}\\right)^2  + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2.$$\n",
    "\n",
    "$\\lambda$ is the term that controls the importance of the regularised term. The minimiser of this cost function usually have very small paramter coefficients ($\\theta_1, \\theta_2, \\ldots, \\theta_n$) to avoid overfitting. Note that we do not penalise $\\theta_0$.\n",
    "\n",
    "### Gradient Decent Algorithm Under Regularisation\n",
    "\n",
    "$$\\boldsymbol{\\theta} := \\left(I - \\lambda\\frac{\\alpha}{m}L\\right)\\boldsymbol{\\theta} - \\frac{\\alpha}{m} X^T \\left(g\\left(X \\boldsymbol{\\theta} \\right) - \\boldsymbol{y}\\right),$$\n",
    "\n",
    "where\n",
    "$$L = \\begin{bmatrix}\n",
    "        0 & & & &\\\\\n",
    "        & 1 & & &\\\\\n",
    "        & & 1 & &\\\\\n",
    "        & & & \\ddots &\\\\\n",
    "        & & & & 1\\\\\n",
    "      \\end{bmatrix}.$$\n",
    "      \n",
    "For linear regression, we use $g(z) = z$, for logistic regression, we use $g(z) = \\frac{1}{1 + \\exp (-z)}$, i.e. the Sigmoid function.\n",
    "\n",
    "### Normal Equation Under Regularisation\n",
    "\n",
    "For linear regression, the normal equation under regularisation is \n",
    "$$\\boldsymbol{\\theta} = \\left(X^T X  + \\lambda L \\right)^{-1} X^T \\boldsymbol{y}.$$\n",
    "These proofs will left for the reader as exercises.\n",
    "\n",
    "Note that under regularisation, $X^T X + \\lambda L$ is always invertible.\n",
    "\n",
    "#### (Extension) Invertibility\n",
    "\n",
    "This requires some linear algebra knowledge about eigenvalues and matrix invertibility.\n",
    "\n",
    "Theoretically, $X^T X$ is either **positive definite** (PD) or **positive semi-definite** (PSD) since \n",
    "$$v^T X^T Xv = \\| Xv \\|^2_2 \\geq 0.$$\n",
    "\n",
    "Then the eigenvalues of $X^T X$ must be non-negative. For any positive $\\lambda$, adding $\\lambda L$ will make all the eigenvalues positive, and positive definte matrix is always invertible.\n",
    "\n",
    "### Advanced Optimisation Methods Under Regularisation\n",
    "\n",
    "Recall our cost function under regularisation is\n",
    "$$J(\\theta) = \\frac{1}{2m} \\sum_{i = 1}^{m}\\left(h_{\\theta} \\left(x^{(i)}\\right) - y^{(i)}\\right)^2  + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2.$$\n",
    "It is easy to show that for $j = 0$,\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_0} J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\left( h_{\\theta} \\left(x^{(i)} \\right) - y^{(i)}\\right)x^{(i)}_0,$$\n",
    "\n",
    "and for $j = 1, 2, \\ldots, n$,\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\left( h_{\\theta} \\left(x^{(i)} \\right) - y^{(i)}\\right)x^{(i)}_j + \\frac{\\lambda}{m} \\theta_j.$$\n",
    "\n",
    "Consequently, this allows us to define the function needed in our usual optimisation procedure.\n",
    "> `function [jVal, gradient] = costFunction(theta)\n",
    "      jVal = [...code to compute J(theta)...];\n",
    "      gradient(0) = [...code to compute derivative of `$\\frac{\\partial}{\\partial \\theta_0}$`...];\n",
    "      gradient(1) = [...code to compute derivative of `$\\frac{\\partial}{\\partial \\theta_1}$`...];\n",
    "      ......\n",
    "      gradient(n) = [...code to compute derivative of `$\\frac{\\partial}{\\partial \\theta_n}$`...];\n",
    "end`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
